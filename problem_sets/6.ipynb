{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: At any fixed value of $x$, $$\\left.\\begin{aligned} \\mathbb { E } \\left( \\widehat { F } _ { n } ( x ) \\right) & = F ( x ) \\\\ \\mathbb { V } \\left( \\widehat { F } _ { n } ( x ) \\right) & = \\frac { F ( x ) ( 1 - F ( x ) ) } { n } , \\\\ \\text { MSE } & = \\frac { F ( x ) ( 1 - F ( x ) ) } { n } \\rightarrow 0 \\end{aligned} \\right.$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of the empirical CDF $$\\widehat { F } _ { n } ( x ) = \\frac { \\sum _ { i = 1 } ^ { n } I \\left( X _ { i } \\leq x \\right) } { n }$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key insight here is that there's an **indicator function** (a boolean valued function) inside there.  Since we sum many results of the indicator function, we have a **binomial distribution** lurking in there as well.  Multipying the empirical CDF by $n$ we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$B = n \\widehat { F } _ { n } ( x ) \\sim \\operatorname { Binomial } ( n , p )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $\\mathbb{E}(B) = np$.  But this is just $P(X \\leq x) = F(x)$, and $$ \\mathbb { E } ( \\widehat { F } ( x ) ) = \\mathbb { E } ( B / n ) = \\frac{1}{n} ( \\mathbb{E}( B ) ) = p = F ( x )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $$X \\sim Binomial(n,p) \\implies \\mathbb{V}(X) = np(p-1), \\\\\\operatorname { Var } ( a X ) = a ^ { 2 } \\operatorname { Var } ( X )$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $$ \\mathbb { V } ( \\hat { F } ( x ) ) = \\mathbb{V}(( B / n )) = (\\frac{1}{n})^2 \\mathbb { V } ( B ) = \\frac{p ( 1 - p )}{ n } = \\frac{ F ( x ) ( 1 - F ( x ) }{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that MSE = (Bias)^2 + Variance.  Since $\\mathbb{E}(\\widehat { F } ( x )) = F ( x )$, we have an unbiased estimator (a bias of 0) and the MSE is exactly equal to the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: $$ \\left. \\begin{array} { l } { \\text { Let } X _ { 1 } , \\ldots , X _ { n } \\sim \\text { Bernoulli(p) and let } Y _ { 1 } , \\ldots , Y _ { m } \\sim \\text { Bernoulli } ( q ) . \\text { Find } } \\\\ { \\text { the plug-in estimator and estimated standard error for } p . \\text { Find an ap- } } \\\\ { \\text { proximate } 90 \\text { percent confidence interval for } p . \\text { Find the plug-in esti- } } \\\\ { \\text { mator and estimated standard error for } p - q . \\text { Find an approximate } 90 } \\\\ { \\text { percent confidence interval for } p - q . } \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{r}$ denote the plug-in estimator for $p - q$.  Then, $$ \\hat { r } = \\int x d \\widehat { F } _ { p } ( x ) - \\int x d \\widehat { F } _ { q } ( x ) = \\overline{X_n} - \\overline{Y_n}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the confidence interval, we further need a standard error and a critical value.  For standard error we have $$\\hat{se} = \\sqrt{\\mathbb{V}(\\hat{r})} = \\mathbb { V } ( \\hat{p} ) + \\mathbb { V } ( \\hat { q } ) = \\sqrt { \\frac { \\widehat { p } ( 1 - \\widehat { p } ) } { n } + \\frac { \\widehat { q } ( 1 - \\widehat { q } ) } { m } }$$ and for the critical value, $$1.64$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entire confidence interval, we have $$\\widehat { p } - \\widehat { q } \\pm 1.64 \\sqrt { \\frac { \\hat { p } ( 1 - \\widehat { p } ) } { n } + \\frac { \\hat { q } ( 1 - \\widehat { q } ) } { m } }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{p}, \\hat{q}$ are the sample means of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Let $X _ { 1 } , \\ldots , X _ { n } \\sim \\operatorname { Poisson } ( \\lambda )$ and let $\\hat { \\lambda } = n ^ { - 1 } \\sum _ { i = 1 } ^ { n } X _ { i }$.  Find the bias, standard error, and MSE of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias $= ( E ( \\hat { \\lambda } ) - \\lambda ) ^ { 2 } = \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E ( \\lambda ) - \\lambda \\right) ^ { 2 } = \\left( \\frac { 1 } { n } \\times ( n \\lambda ) - \\lambda \\right) ^ { 2 } = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{V}(\\hat{Î³}) = \\mathbb { V } \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i } \\right) = \\frac { 1 } { n ^ { 2 } } \\mathbb { V } \\left( \\sum _ { i = 1 } ^ { n } X _ { i } \\right) = \\frac { 1 } { n _ { i } ^ { 2 } } \\sum _ { i = 1 } ^ { n } \\mathbb { V } \\left( X _ { i } \\right) = \\frac { 1 } { n ^ { 2 } } \\sum _ { i = 1 } ^ { n } \\lambda = \\frac { \\lambda } { n } \\implies \\\\ \\hat{se} = \\frac{\\sqrt{\\gamma}}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the bias variance decomposition that $$MSE = bias^2 + variance = 0^2 + \\frac { \\lambda } { n } = \\frac { \\lambda } { n }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Let $X _ { 1 } , \\ldots , X _ { n } \\sim \\text { Uniform } ( 0 , \\theta )$ and let $\\hat { \\theta } = \\max \\left\\{ X _ { 1 } , \\ldots , X _ { n } \\right\\}$.  Find the bias, standard error, and MSE of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're estimating parameters of uniform using a couple different test statistics and noting the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know the variance of the `max` function in closed form, so let's find the CDF and integrate to get PDF\n",
    "\n",
    "then we can get expectation\n",
    "\n",
    "we'll use expectation to get bias\n",
    "\n",
    "we need bias\n",
    "\n",
    "we can get standard error from expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Let $X _ { 1 } , \\ldots , X _ { n } \\sim \\text { Uniform } ( 0 , \\theta )$ and let $\\hat { \\theta } = 2 \\overline { X } _ { n }$.  Find the bias, standard error, and MSE of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find a closed form solution for expectation and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Let $X _ { 1 } , \\ldots , X _ { n } \\sim \\text { Uniform } ( 0,1 )$.  Let $Y _ { n } = \\overline { X } _ { n } ^ { 2 }$.  Find the limiting distribution of $Y_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the delta method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Let $$ \\left( \\begin{array} { c } { X _ { 11 } } \\\\ { X _ { 21 } } \\end{array} \\right) , \\left( \\begin{array} { c } { X _ { 12 } } \\\\ { X _ { 22 } } \\end{array} \\right) , \\ldots , \\left( \\begin{array} { c } { X _ { 1 n } } \\\\ { X _ { 2 n } } \\end{array} \\right)$$ be iid random vectors with mean $\\mu = \\left( \\mu _ { 1 } , \\mu _ { 2 } \\right)$ and variance $\\Sigma$.  Let $$ \\overline { X } _ { 1 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } X _ { 1 i } , \\quad \\overline { X } _ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } X _ { 2 i }$$ and define $ Y _ { n } = \\overline { X } _ { 1 } / \\overline { X } _ { 2 }$.  Find the limiting distribution of $Y_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again use the delta method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
