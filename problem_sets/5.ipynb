{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: $$ \\left. \\begin{array} { l } { \\text { Let } X \\sim \\text { Exponential(\\beta). Find } \\mathbb { P } \\left( \\left| X - \\mu _ { X } \\right| \\geq k \\sigma _ { X } \\right) \\text { for } k > 1 . \\text { Compare } } \\\\ { \\text { this to the bound you get from Chebyshev's inequality. } } \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: $$ \\left. \\begin{array} { l } { \\text { Let } X \\sim \\text { Poisson } ( \\lambda ) . \\text { Use Chebyshev's inequality to show that } \\mathbb { P } ( X \\geq } \\\\ { 2 \\lambda ) \\leq 1 / \\lambda . } \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, both the **expectation** and **variance** of a Poisson distribution are $γ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, $$ P ( x \\geq 2 \\lambda ) = P ( x - \\lambda \\geq \\lambda ) \\leq P ( | x - \\lambda | \\geq \\lambda ) \\leq \\frac { \\lambda } { \\lambda^{2} } = \\frac { 1 } { \\lambda }$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Let $X _ { 1 } , \\ldots , X _ { n } \\sim \\text { Bernoulli } ( p )$.  Let $α > 0$ be fixed and define $$\\epsilon _ { n } = \\sqrt { \\frac { 1 } { 2 n } \\log \\left( \\frac { 2 } { \\alpha } \\right) }$$.  Let $$\\widehat { p } _ { n } = n ^ { - 1 } \\sum _ { i = 1 } ^ { n } X _ { i }$$.  Define $$C _ { n } = \\left( \\hat { p } _ { n } - \\epsilon _ { n } , \\widehat { p } _ { n } + \\epsilon _ { n } \\right)$$.  Use Hoeffding's Inequality to show that $$ \\mathbb { P } \\left( p \\in C_{n} \\right) \\geq 1 - \\alpha$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note that $\\hat{p}_n$ is really just the definition of the sample average, so $$ p \\in C _ { n } \\Rightarrow \\\\ | \\hat { p } - p | < \\varepsilon _ { n } \\Rightarrow \\\\ | \\overline { X }_n - p | < \\epsilon _ { n }$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of this happening is $$ \\mathbb{P}(\\left| \\overline { X } _ { n } - p \\right| < \\epsilon _ { n }) = \\\\ 1 - \\mathbb { P } \\left( \\left| \\overline { X } _ { n } - p \\right| \\geq \\epsilon _ { n } \\right) \\leq 1 - α$$ by Hoeffding's Inequality, plus a lot of tedious algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: Show that a) the expectation of the sample variance is the variance b) the sample variance converges in probability to the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: $$\\left. \\begin{array} { c } { \\text { Let } X _ { 1 } , X _ { 2 } , \\ldots \\text { be a sequence of random variables. Show that } X _ { n } \\stackrel { \\mathrm { qm } } { \\rightarrow } b } \\\\ { \\text { if and only if } } \\\\ { \\lim _ { n \\rightarrow \\infty } \\mathbb { E } \\left( X _ { n } \\right) = b \\quad \\text { and } \\quad \\lim _ { n \\rightarrow \\infty } \\mathbb { V } \\left( X _ { n } \\right) = 0 } \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $\\lim _ { n \\rightarrow \\infty } \\mathbb { E } \\left( X _ { n } \\right) = b$, $\\lim _ { n \\rightarrow \\infty } \\mathbb { V } \\left( X _ { n } \\right) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, $$\\big( \\mathbb { E } \\left( X _ { n } \\right) - b \\big) ^ 2 + \\mathbb{V}(X_n) \\xrightarrow{} 0 \\implies \\\\ \\left( \\mathbb { E } \\left( X _ { n } \\right) - b \\right) ^ { 2 } + \\mathbb { E } \\big[ \\left( X _ { n } - \\mathbb { E } \\left( X _ { n } \\right) \\right) ^ { 2 } \\big] \\implies \\\\ \\mathbb { E } \\left[ \\left( X _ { n } - \\mathbb { E } \\left( X _ { n } \\right) \\right) ^ { 2 } + \\left( \\mathbb { E } \\left( X _ { n } \\right) - b \\right) ^ { 2 } + 2 \\left( X _ { n } - \\mathbb { E } \\left( X _ { n } \\right) \\right) \\left( \\mathbb { E } \\left( X _ { n } \\right) - b \\right) \\right]$$ since we can add any terms we like as long as they are equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this equals $$ \\mathbb { E } \\left( X _ { n } - \\mathbb { E } \\left( X _ { n } \\right) + \\mathbb { E } \\left( X _ { n } \\right) - b \\right) ^ { 2 } = \\mathbb { E } \\left( X _ { n } - b \\right) ^ { 2 }$$ and we've proven necessity.  The proof of sufficiency is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: $$ \\left. \\begin{array} { c } { \\text { Let } X _ { 1 } , X _ { 2 } , \\ldots \\text { be a sequence of random variables such that } } \\\\ { \\mathbb { P } \\left( X _ { n } = \\frac { 1 } { n } \\right) = 1 - \\frac { 1 } { n ^ { 2 } } \\text { and } \\mathbb { P } \\left( X _ { n } = n \\right) = \\frac { 1 } { n ^ { 2 } } } \\\\ { \\text { Does } X _ { n } \\text { converge in probability? Does } X _ { n } \\text { converge in quadratic mean? } } \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: $$ \\left. \\begin{array} { r l } { \\text { Let } X _ { 1 } , \\ldots , X _ { n } \\sim \\text { Bernoulli } ( p ) . \\text { Prove that } } \\\\ { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } X _ { i } ^ { 2 } \\stackrel { \\mathrm { P } } { \\longrightarrow } p \\quad \\text { and } } & { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } X _ { i } ^ { 2 } \\stackrel { \\mathrm { qm } } { \\longrightarrow } p } \\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only have to show that $Y_i = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } X _ { i } ^ { 2 }$ converges in quadratic mean to $p$, since that will imply that it converges in probability to $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show convergence to quadratic mean, recall that it's enough to show $$ \\mathbb{V}(Y_i) = 0, \\mathbb{E}(Y_i) = p$$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\mathbb { E } \\left( Y _ { i } \\right) = p$\n",
    "The expectation of one of the terms is just $$ \\mathbb { \\mathbb{E} } \\left( X _ { i } ^ { 2 } \\right) = 1^2(p) + 0^2(1-p) = p$$ so $$ \\mathbb{E} \\left( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } X _ { i } ^ { 2 } \\right) = \\\\ \\left( \\frac { 1 } { n } \\right) \\times ( n ) \\mathbb{E} \\left( x _ { i } ^ { 2 } \\right) = \\\\ \\mathbb{E} \\left( X _ { i } ^ { 2 } \\right) = \\\\ p$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\mathbb { V } \\left( Y _ { i } \\right) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of one of the terms is just $$\\mathbb { V } \\left( X _ { i } ^ { 2 } \\right) = \\mathbb { E } \\left( X _ { i } ^ { 4 } \\right) - \\left( \\mathbb { E } \\left( X _ { i } ^ { 2 } \\right) \\right) ^ { 2 } = p - p ^ { 2 }$$ so $$ V \\left( Y _ { i } \\right) = \\frac { p ^ { 2 } - p } { n } \\rightarrow 0$$ and we have convergence in quadratic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppose that $\\mathbb { P } ( X = 1 ) = \\mathbb { P } ( X = - 1 ) = 1 / 2$.  Define $$ X _ { n } = \\left\\{ \\begin{array} { l l } { X } & { \\text { with probability } 1 - \\frac { 1 } { n } } \\\\ { e ^ { n } } & { \\text { with probability } \\frac { 1 } { n } . } \\end{array} \\right. $$.  Does $X_n$ converge to $X$ in probability?  Does $\\mathbb { E } \\left( X - X _ { n } \\right) ^ { 2 }$ converge to 0? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence in probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the transformation $X _ { n } - X$.  There are 4 outcomes of this \"experiment\"\n",
    "$$ \\text{result of } X _ { n } - X= \\left\\{ \\begin{array} { l l l l} { X = 1, X_n = X } & { \\text { with probability } (\\frac{1}{2})(1 - \\frac { 1 } { n }) } \\\\ { X = -1, X_n = X  } & { \\text { with probability } (\\frac{1}{2})(1 - \\frac { 1 } { n } }) \\\\ {X = 1, X_n = e^{n} } & { \\text {with probability } (\\frac{1}{2})\\frac { 1 } { n }} \\\\ {X = -1, X_n = e^{n} } & {\\text {with probability } (\\frac{1}{2})\\frac { 1 } { n }} \\end{array} \\right.$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this simplifies to $$X _ { n } - X = \\left\\{ \\begin{array} { l l } { 0 } & { \\text { with probability } 1 - \\frac { 1 } { n } } \\\\ { e ^ { n } - X} & { \\text { with probability } \\frac { 1 } { n } . } \\end{array} \\right.$$ so $$ \\mathbb { P } \\left( \\left| X _ { n } - X \\right| > \\epsilon > 0\\right) = \\frac{1}{n}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequence of probabilities $\\frac{1}{n}$ converges to 0, so $X_n$ converges to $X$ in probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence of L2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the 4 different outcomes outlined above.  Then, $$ \n",
    "\\mathbb{E}[(X - X_n)^2] = \\left( 1 - e ^ { n } \\right) ^ { 2 } \\times \\left( \\frac { 1 } { 2 } \\right) \\times \\left( \\frac { 1 } { n } \\right) + \\left( - 1 - e ^ { n } \\right) ^ { 2 } \\times \\left( \\frac { 1 } { 2 } \\right) \\times \\left( \\frac { 1 } { n } \\right) \\xrightarrow{} \\infty$$ since the exponentials are in the numerator and the linear are in the denominator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
